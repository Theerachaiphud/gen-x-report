Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Lmod is automatically replacing "cce/16.0.1" with "gcc/12.2.0".


Lmod is automatically replacing "PrgEnv-cray/8.4.0" with "PrgEnv-gnu/8.4.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.27


The following have been reloaded with a version change:
  1) PrgEnv-gnu/8.4.0 => PrgEnv-gnu/8.3.3
  2) cray-libsci/23.09.1.1 => cray-libsci/23.02.1.1
  3) cray-mpich/8.1.27 => cray-mpich/8.1.25
  4) craype/2.7.23 => craype/2.7.20
  5) gcc/12.2.0 => gcc/11.2.0
  6) perftools-base/23.09.0 => perftools-base/23.03.0

SLURM enviroment
SLURM_MPI_TYPE=cray_shasta
SLURM_STEP_ID=1
SLURM_STEP_GPUS=0,1,2,3
SLURM_NODEID=2
SLURM_TASK_PID=19026
SLURM_PRIO_PROCESS=0
SLURM_CPU_BIND_VERBOSE=quiet
SLURM_SUBMIT_DIR=/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/Encoder-Decoder
SLURM_CPUS_PER_TASK=64
SLURM_STEPID=1
SLURM_SRUN_COMM_HOST=10.168.0.86
SLURM_DISTRIBUTION=cyclic
SLURM_GPUS_PER_NODE=4
SLURM_PROCID=2
SLURM_JOB_GID=1980402197
SLURM_CPU_BIND=quiet,mask_cpu:0xFFFFFFFFFFFFFFFF
SLURMD_NODENAME=lanta-g-024
SLURM_TASKS_PER_NODE=1(x4)
SLURM_NNODES=4
SLURM_LAUNCH_NODE_IPADDR=10.168.0.86
SLURM_STEP_TASKS_PER_NODE=1(x4)
SLURM_JOB_NODELIST=lanta-g-[020,023-025]
SLURM_CLUSTER_NAME=lanta
SLURM_NODELIST=lanta-g-[020,023-025]
SLURM_GPUS_ON_NODE=4
SLURM_NTASKS=4
SLURM_UMASK=0022
SLURM_JOB_CPUS_PER_NODE=64(x4)
SLURM_TOPOLOGY_ADDR=global.group2.lanta-g-024
SLURM_WORKING_CLUSTER=lanta:x3003c0s21b0n0:6817:9728:109
SLURM_STEP_NODELIST=lanta-g-[020,023-025]
SLURM_JOB_NAME=gen-x-report
SLURM_SRUN_COMM_PORT=41417
SLURM_JOB_GPUS=0,1,2,3
SLURM_JOBID=1989816
SLURM_CONF=/var/spool/slurmd/conf-cache/slurm.conf
SLURM_NODE_ALIASES=(null)
SLURM_JOB_QOS=aimedi
SLURM_TOPOLOGY_ADDR_PATTERN=switch.switch.node
SLURM_STEP_RESV_PORTS=21886-21887
SLURM_CPUS_ON_NODE=64
SLURM_JOB_NUM_NODES=4
SLURM_JOB_UID=1980402197
SLURM_JOB_PARTITION=gpu
SLURM_CPU_BIND_LIST=0xFFFFFFFFFFFFFFFF
SLURM_JOB_USER=tpornsur
SLURM_NPROCS=4
SLURM_SUBMIT_HOST=x3002c0s7b0n0
SLURM_JOB_ACCOUNT=lt200203
SLURM_STEP_LAUNCHER_PORT=41417
SLURM_GTIDS=2
SLURM_JOB_ID=1989816
SLURM_CPU_BIND_TYPE=mask_cpu:
SLURM_STEP_NUM_TASKS=4
SLURM_JOB_CPUS_PER_NODE_PACK_GROUP_0=64(x4)
SLURM_STEP_NUM_NODES=4
SLURM_LOCALID=0
Python Version:
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/bin/python
Python 3.12.5
Torch Availability:
Torch available: True
Conda Environment:                       *  /lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1
MASTER_ADDR = lanta-g-020
MASTER_PORT = 17314
num_processes 16
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[W106 00:19:55.275065051 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:55.276299936 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:55.277515844 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:55.277596519 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:55.278322121 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:55.278406654 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:55.282714714 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:55.282796151 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.33it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.24it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.31it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.01it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.89it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.96it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.95it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00, 11.19it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00, 11.12it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 11.33it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00, 11.72it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00, 11.84it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 11.71it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.06it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.90it/s]
Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.19it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.01it/s]
Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.66it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.60it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00, 11.08it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00, 11.62it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.97it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.81it/s]
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-01-06 00:20:25,944] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:25,964] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:25,964] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:27,009] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-06 00:20:27,016] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-06 00:20:27,016] [INFO] [comm.py:652:init_distributed] cdb=None
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-01-06 00:20:28,912] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:29,794] [INFO] [comm.py:652:init_distributed] cdb=None
x1000c3s3b0n1:27936:27936 [0] NCCL INFO cudaDriverVersion 12000
x1000c3s3b0n1:27937:27937 [1] NCCL INFO cudaDriverVersion 12000
x1000c3s3b0n1:27938:27938 [2] NCCL INFO cudaDriverVersion 12000
x1000c3s3b0n1:27939:27939 [3] NCCL INFO cudaDriverVersion 12000
x1000c3s3b0n1:27936:27936 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27937:27937 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27938:27938 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27939:27939 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27938:27938 [2] NCCL INFO Bootstrap : Using hsn0:10.150.1.56<0>
x1000c3s3b0n1:27937:27937 [1] NCCL INFO Bootstrap : Using hsn0:10.150.1.56<0>
x1000c3s3b0n1:27936:27936 [0] NCCL INFO Bootstrap : Using hsn0:10.150.1.56<0>
x1000c3s3b0n1:27939:27939 [3] NCCL INFO Bootstrap : Using hsn0:10.150.1.56<0>
x1000c3s3b0n1:27936:27936 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s3b0n1:27938:27938 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s3b0n1:27937:27937 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s3b0n1:27939:27939 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s3b0n1:27939:6001 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27939:6001 [3] NCCL INFO NET/IB : No device found.
x1000c3s3b0n1:27939:6001 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27939:6001 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.56<0> [1]hsn1:10.150.1.64<0>
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Using network Socket
x1000c3s3b0n1:27936:6002 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27938:6004 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27937:6003 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27936:6002 [0] NCCL INFO NET/IB : No device found.
x1000c3s3b0n1:27938:6004 [2] NCCL INFO NET/IB : No device found.
x1000c3s3b0n1:27937:6003 [1] NCCL INFO NET/IB : No device found.
x1000c3s3b0n1:27936:6002 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27938:6004 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27937:6003 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n1:27937:6003 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.56<0> [1]hsn1:10.150.1.64<0>
x1000c3s3b0n1:27938:6004 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.56<0> [1]hsn1:10.150.1.64<0>
x1000c3s3b0n1:27936:6002 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.56<0> [1]hsn1:10.150.1.64<0>
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Using network Socket
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Using network Socket
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Using network Socket
x1000c3s3b0n1:27937:6003 [1] NCCL INFO comm 0x5563a9f74c20 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s3b0n1:27936:6002 [0] NCCL INFO comm 0x55cb168ebe40 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s3b0n1:27939:6001 [3] NCCL INFO comm 0x55fcaba38520 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s3b0n1:27938:6004 [2] NCCL INFO comm 0x55bc94da9780 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s3b0n1:27937:6003 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Setting affinity for GPU 1 to ffff,00000000
x1000c3s3b0n1:27939:6001 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Setting affinity for GPU 3 to ffff
x1000c3s3b0n1:27936:6002 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000
x1000c3s3b0n1:27938:6004 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
x1000c3s3b0n1:27937:6003 [1] NCCL INFO comm 0x5563a9f74c20 rank 9 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
x1000c3s3b0n1:27939:6001 [3] NCCL INFO comm 0x55fcaba38520 rank 11 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
x1000c3s3b0n1:27938:6004 [2] NCCL INFO comm 0x55bc94da9780 rank 10 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
x1000c3s3b0n1:27936:6002 [0] NCCL INFO comm 0x55cb168ebe40 rank 8 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Trees [0] 10/-1/-1->9->11 [1] 10/-1/-1->9->8 [2] 10/-1/-1->9->11 [3] 10/-1/-1->9->8
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Trees [0] 9/4/-1->11->8 [1] 8/15/-1->11->3 [2] 9/-1/-1->11->8 [3] 8/-1/-1->11->4
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Trees [0] -1/-1/-1->10->9 [1] -1/-1/-1->10->9 [2] -1/-1/-1->10->9 [3] -1/-1/-1->10->9
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Trees [0] 11/12/-1->8->0 [1] 9/7/-1->8->11 [2] 11/-1/-1->8->7 [3] 9/-1/-1->8->11
x1000c3s3b0n1:27937:6003 [1] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n1:27939:6001 [3] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n1:27938:6004 [2] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n1:27936:6002 [0] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n1:27936:16464 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 2.
x1000c3s3b0n1:27936:16464 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
x1000c3s3b0n1:27936:16464 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 00/0 : 7[3] -> 8[0] [receive] via NET/Socket/1
x1000c3s3b0n1:27936:16464 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 02/0 : 7[3] -> 8[0] [receive] via NET/Socket/1
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 00 : 8[0] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 02 : 8[0] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Channel 00 : 10[2] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Channel 00 : 9[1] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Channel 02 : 10[2] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Channel 02 : 9[1] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 02/0 : 11[3] -> 12[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:16465 [3] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 2.
x1000c3s3b0n1:27939:16465 [3] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
x1000c3s3b0n1:27939:16465 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 01/0 : 4[0] -> 11[3] [receive] via NET/Socket/0
x1000c3s3b0n1:27939:16465 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 03/0 : 4[0] -> 11[3] [receive] via NET/Socket/0
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Channel 01 : 9[1] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Channel 03 : 9[1] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Channel 01 : 10[2] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Channel 03 : 10[2] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 01 : 11[3] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 03 : 11[3] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[3] [send] via NET/Socket/0
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[3] [send] via NET/Socket/0
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Connected all rings
x1000c3s3b0n1:27936:16464 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 01/0 : 7[3] -> 8[0] [receive] via NET/Socket/0
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Connected all rings
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 01 : 8[0] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 03 : 8[0] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Connected all rings
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Connected all rings
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Channel 01 : 9[1] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Channel 03 : 9[1] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 00 : 8[0] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Channel 00 : 10[2] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Channel 00 : 9[1] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 01 : 8[0] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Channel 02 : 10[2] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Channel 02 : 9[1] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 02 : 8[0] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 03 : 8[0] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 01/0 : 11[3] -> 15[3] [send] via NET/Socket/0
x1000c3s3b0n1:27936:16464 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/Socket/1
x1000c3s3b0n1:27939:16465 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 00/0 : 4[0] -> 11[3] [receive] via NET/Socket/1
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:16465 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [receive] via NET/Socket/0
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [send] via NET/Socket/0
x1000c3s3b0n1:27936:16464 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/Socket/1
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 00/0 : 11[3] -> 4[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 03/0 : 11[3] -> 4[0] [send] via NET/Socket/0
x1000c3s3b0n1:27939:16465 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 01/0 : 15[3] -> 11[3] [receive] via NET/Socket/0
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 00 : 11[3] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 01 : 11[3] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 02 : 11[3] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 03 : 11[3] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 00 : 11[3] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 01/0 : 8[0] -> 7[3] [send] via NET/Socket/0
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Channel 02 : 11[3] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Channel 02/0 : 8[0] -> 7[3] [send] via NET/Socket/1
x1000c3s3b0n1:27939:6001 [3] NCCL INFO Connected all trees
x1000c3s3b0n1:27936:6002 [0] NCCL INFO Connected all trees
x1000c3s3b0n1:27938:6004 [2] NCCL INFO Connected all trees
x1000c3s3b0n1:27937:6003 [1] NCCL INFO Connected all trees
x1000c3s3b0n1:27939:6001 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n1:27936:6002 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n1:27938:6004 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n1:27939:6001 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n1:27937:6003 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n1:27936:6002 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n1:27938:6004 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n1:27937:6003 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n1:27937:6003 [1] NCCL INFO comm 0x5563a9f74c20 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s3b0n1:27939:6001 [3] NCCL INFO comm 0x55fcaba38520 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s3b0n1:27936:6002 [0] NCCL INFO comm 0x55cb168ebe40 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s3b0n1:27938:6004 [2] NCCL INFO comm 0x55bc94da9780 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
Error opening image: /project/lt200203-aimedi/mimix-cxr/mimic-cxr-jpg/mimic-cxr-jpg/files/p10/p10160202/s54843043/f34377de-fec7d46c-0c959d61-97f0a15e-061466c2.jpg, image file is truncated (6 bytes not processed)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Error opening image: /project/lt200203-aimedi/mimix-cxr/mimic-cxr-jpg/mimic-cxr-jpg/files/p14/p14145825/s57512369/adc5ede3-ca5837f1-b24f2d4c-ddee77bd-5b7c2aab.jpg, image file is truncated (16 bytes not processed)
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Using network Socket
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Using network Socket
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Using network Socket
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Using network Socket
x1000c3s3b0n1:27939:14710 [3] NCCL INFO comm 0x55fd53bacd50 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0x4eadd15c38937d75 - Init START
x1000c3s3b0n1:27938:14712 [2] NCCL INFO comm 0x55bd3e0de340 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0x4eadd15c38937d75 - Init START
x1000c3s3b0n1:27937:14711 [1] NCCL INFO comm 0x556455950580 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0x4eadd15c38937d75 - Init START
x1000c3s3b0n1:27936:14709 [0] NCCL INFO comm 0x55cbc1f50210 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0x4eadd15c38937d75 - Init START
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Setting affinity for GPU 3 to ffff
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Setting affinity for GPU 1 to ffff,00000000
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000
x1000c3s3b0n1:27939:14710 [3] NCCL INFO comm 0x55fd53bacd50 rank 11 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
x1000c3s3b0n1:27938:14712 [2] NCCL INFO comm 0x55bd3e0de340 rank 10 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
x1000c3s3b0n1:27936:14709 [0] NCCL INFO comm 0x55cbc1f50210 rank 8 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
x1000c3s3b0n1:27937:14711 [1] NCCL INFO comm 0x556455950580 rank 9 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Trees [0] 9/4/-1->11->8 [1] 8/15/-1->11->3 [2] 9/-1/-1->11->8 [3] 8/-1/-1->11->4
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Trees [0] -1/-1/-1->10->9 [1] -1/-1/-1->10->9 [2] -1/-1/-1->10->9 [3] -1/-1/-1->10->9
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Trees [0] 11/12/-1->8->0 [1] 9/7/-1->8->11 [2] 11/-1/-1->8->7 [3] 9/-1/-1->8->11
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Trees [0] 10/-1/-1->9->11 [1] 10/-1/-1->9->8 [2] 10/-1/-1->9->11 [3] 10/-1/-1->9->8
x1000c3s3b0n1:27939:14710 [3] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n1:27938:14712 [2] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n1:27936:14709 [0] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n1:27937:14711 [1] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n1:27936:14923 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 00/0 : 7[3] -> 8[0] [receive] via NET/Socket/1
x1000c3s3b0n1:27936:14923 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 02/0 : 7[3] -> 8[0] [receive] via NET/Socket/1
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 00 : 8[0] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 02 : 8[0] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Channel 00 : 9[1] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Channel 00 : 10[2] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Channel 02 : 9[1] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Channel 02 : 10[2] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 02/0 : 11[3] -> 12[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:14922 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 01/0 : 4[0] -> 11[3] [receive] via NET/Socket/0
x1000c3s3b0n1:27939:14922 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 03/0 : 4[0] -> 11[3] [receive] via NET/Socket/0
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[3] [send] via NET/Socket/0
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[3] [send] via NET/Socket/0
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 01 : 11[3] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 03 : 11[3] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Channel 01 : 9[1] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Channel 03 : 9[1] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Channel 01 : 10[2] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Channel 03 : 10[2] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Connected all rings
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Connected all rings
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Connected all rings
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Connected all rings
x1000c3s3b0n1:27936:14923 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 01/0 : 7[3] -> 8[0] [receive] via NET/Socket/0
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 01 : 8[0] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 03 : 8[0] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Channel 01 : 9[1] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Channel 03 : 9[1] -> 10[2] via SHM/direct/direct
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 00 : 8[0] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Channel 00 : 10[2] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Channel 00 : 9[1] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 01 : 8[0] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Channel 02 : 10[2] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Channel 02 : 9[1] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 02 : 8[0] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 03 : 8[0] -> 11[3] via SHM/direct/direct
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 01/0 : 11[3] -> 15[3] [send] via NET/Socket/0
x1000c3s3b0n1:27936:14923 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [receive] via NET/Socket/1
x1000c3s3b0n1:27939:14922 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 00/0 : 4[0] -> 11[3] [receive] via NET/Socket/1
x1000c3s3b0n1:27936:14923 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:14922 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/Socket/1
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [receive] via NET/Socket/0
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [send] via NET/Socket/0
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 00/0 : 11[3] -> 4[0] [send] via NET/Socket/1
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 03/0 : 11[3] -> 4[0] [send] via NET/Socket/0
x1000c3s3b0n1:27939:14922 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 01/0 : 15[3] -> 11[3] [receive] via NET/Socket/0
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 00 : 11[3] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 01 : 11[3] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 02 : 11[3] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 03 : 11[3] -> 8[0] via SHM/direct/direct
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 00 : 11[3] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 01/0 : 8[0] -> 7[3] [send] via NET/Socket/0
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Channel 02/0 : 8[0] -> 7[3] [send] via NET/Socket/1
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Channel 02 : 11[3] -> 9[1] via SHM/direct/direct
x1000c3s3b0n1:27939:14710 [3] NCCL INFO Connected all trees
x1000c3s3b0n1:27936:14709 [0] NCCL INFO Connected all trees
x1000c3s3b0n1:27939:14710 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n1:27937:14711 [1] NCCL INFO Connected all trees
x1000c3s3b0n1:27938:14712 [2] NCCL INFO Connected all trees
x1000c3s3b0n1:27936:14709 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n1:27939:14710 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n1:27937:14711 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n1:27938:14712 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n1:27936:14709 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n1:27937:14711 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n1:27938:14712 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n1:27936:14709 [0] NCCL INFO comm 0x55cbc1f50210 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s3b0n1:27938:14712 [2] NCCL INFO comm 0x55bd3e0de340 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s3b0n1:27939:14710 [3] NCCL INFO comm 0x55fd53bacd50 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s3b0n1:27937:14711 [1] NCCL INFO comm 0x556455950580 rank 9 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0x4eadd15c38937d75 - Init COMPLETE
