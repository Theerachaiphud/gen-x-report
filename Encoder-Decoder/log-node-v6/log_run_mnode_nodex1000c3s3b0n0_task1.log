Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Lmod is automatically replacing "cce/16.0.1" with "gcc/12.2.0".


Lmod is automatically replacing "PrgEnv-cray/8.4.0" with "PrgEnv-gnu/8.4.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.27


The following have been reloaded with a version change:
  1) PrgEnv-gnu/8.4.0 => PrgEnv-gnu/8.3.3
  2) cray-libsci/23.09.1.1 => cray-libsci/23.02.1.1
  3) cray-mpich/8.1.27 => cray-mpich/8.1.25
  4) craype/2.7.23 => craype/2.7.20
  5) gcc/12.2.0 => gcc/11.2.0
  6) perftools-base/23.09.0 => perftools-base/23.03.0

SLURM enviroment
SLURM_MPI_TYPE=cray_shasta
SLURM_STEP_ID=1
SLURM_STEP_GPUS=0,1,2,3
SLURM_NODEID=1
SLURM_TASK_PID=15064
SLURM_PRIO_PROCESS=0
SLURM_CPU_BIND_VERBOSE=quiet
SLURM_SUBMIT_DIR=/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/Encoder-Decoder
SLURM_CPUS_PER_TASK=64
SLURM_STEPID=1
SLURM_SRUN_COMM_HOST=10.168.0.86
SLURM_DISTRIBUTION=cyclic
SLURM_GPUS_PER_NODE=4
SLURM_PROCID=1
SLURM_JOB_GID=1980402197
SLURM_CPU_BIND=quiet,mask_cpu:0xFFFFFFFFFFFFFFFF
SLURMD_NODENAME=lanta-g-023
SLURM_TASKS_PER_NODE=1(x4)
SLURM_NNODES=4
SLURM_LAUNCH_NODE_IPADDR=10.168.0.86
SLURM_STEP_TASKS_PER_NODE=1(x4)
SLURM_JOB_NODELIST=lanta-g-[020,023-025]
SLURM_CLUSTER_NAME=lanta
SLURM_NODELIST=lanta-g-[020,023-025]
SLURM_GPUS_ON_NODE=4
SLURM_NTASKS=4
SLURM_UMASK=0022
SLURM_JOB_CPUS_PER_NODE=64(x4)
SLURM_TOPOLOGY_ADDR=global.group2.lanta-g-023
SLURM_WORKING_CLUSTER=lanta:x3003c0s21b0n0:6817:9728:109
SLURM_STEP_NODELIST=lanta-g-[020,023-025]
SLURM_JOB_NAME=gen-x-report
SLURM_SRUN_COMM_PORT=41417
SLURM_JOB_GPUS=0,1,2,3
SLURM_JOBID=1989816
SLURM_CONF=/var/spool/slurmd/conf-cache/slurm.conf
SLURM_NODE_ALIASES=(null)
SLURM_JOB_QOS=aimedi
SLURM_TOPOLOGY_ADDR_PATTERN=switch.switch.node
SLURM_STEP_RESV_PORTS=21886-21887
SLURM_CPUS_ON_NODE=64
SLURM_JOB_NUM_NODES=4
SLURM_JOB_UID=1980402197
SLURM_JOB_PARTITION=gpu
SLURM_CPU_BIND_LIST=0xFFFFFFFFFFFFFFFF
SLURM_JOB_USER=tpornsur
SLURM_NPROCS=4
SLURM_SUBMIT_HOST=x3002c0s7b0n0
SLURM_JOB_ACCOUNT=lt200203
SLURM_STEP_LAUNCHER_PORT=41417
SLURM_GTIDS=1
SLURM_JOB_ID=1989816
SLURM_CPU_BIND_TYPE=mask_cpu:
SLURM_STEP_NUM_TASKS=4
SLURM_JOB_CPUS_PER_NODE_PACK_GROUP_0=64(x4)
SLURM_STEP_NUM_NODES=4
SLURM_LOCALID=0
Python Version:
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/bin/python
Python 3.12.5
Torch Availability:
Torch available: True
Conda Environment:                       *  /lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1
MASTER_ADDR = lanta-g-020
MASTER_PORT = 17314
num_processes 16
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[W106 00:19:59.476865672 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:59.478354086 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:59.508214347 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:59.487702812 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:59.508256940 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:59.487749823 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:59.508324109 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:59.508391650 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.54it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.50it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.52it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.48it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.25it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.23it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.23it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.20it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.76it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.77it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.71it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.78it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 11.34it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 11.24it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 11.17it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 11.17it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 11.80it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 11.73it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 11.65it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 11.49it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.74it/s]
Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.69it/s]
Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.61it/s]
Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.46it/s]
train: 65720
eval: 16431
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-01-06 00:20:30,850] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:30,850] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:30,850] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-01-06 00:20:30,938] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:32,842] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-06 00:20:32,842] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-06 00:20:32,842] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-06 00:20:32,842] [INFO] [comm.py:652:init_distributed] cdb=None
x1000c3s3b0n0:24102:24102 [3] NCCL INFO cudaDriverVersion 12000
x1000c3s3b0n0:24101:24101 [2] NCCL INFO cudaDriverVersion 12000
x1000c3s3b0n0:24099:24099 [0] NCCL INFO cudaDriverVersion 12000
x1000c3s3b0n0:24100:24100 [1] NCCL INFO cudaDriverVersion 12000
x1000c3s3b0n0:24102:24102 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24101:24101 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24100:24100 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24099:24099 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24102:24102 [3] NCCL INFO Bootstrap : Using hsn0:10.150.1.68<0>
x1000c3s3b0n0:24101:24101 [2] NCCL INFO Bootstrap : Using hsn0:10.150.1.68<0>
x1000c3s3b0n0:24100:24100 [1] NCCL INFO Bootstrap : Using hsn0:10.150.1.68<0>
x1000c3s3b0n0:24099:24099 [0] NCCL INFO Bootstrap : Using hsn0:10.150.1.68<0>
x1000c3s3b0n0:24102:24102 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s3b0n0:24099:24099 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s3b0n0:24101:24101 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s3b0n0:24100:24100 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s3b0n0:24102:55014 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24100:55016 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24099:55015 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24101:55017 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24099:55015 [0] NCCL INFO NET/IB : No device found.
x1000c3s3b0n0:24100:55016 [1] NCCL INFO NET/IB : No device found.
x1000c3s3b0n0:24102:55014 [3] NCCL INFO NET/IB : No device found.
x1000c3s3b0n0:24101:55017 [2] NCCL INFO NET/IB : No device found.
x1000c3s3b0n0:24099:55015 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24100:55016 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24102:55014 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24101:55017 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s3b0n0:24100:55016 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.68<0> [1]hsn1:10.150.1.63<0>
x1000c3s3b0n0:24102:55014 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.68<0> [1]hsn1:10.150.1.63<0>
x1000c3s3b0n0:24099:55015 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.68<0> [1]hsn1:10.150.1.63<0>
x1000c3s3b0n0:24101:55017 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.68<0> [1]hsn1:10.150.1.63<0>
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Using network Socket
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Using network Socket
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Using network Socket
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Using network Socket
x1000c3s3b0n0:24102:55014 [3] NCCL INFO comm 0x560ca8e79970 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s3b0n0:24099:55015 [0] NCCL INFO comm 0x55ac4e7cb6a0 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s3b0n0:24101:55017 [2] NCCL INFO comm 0x55648226ad10 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s3b0n0:24100:55016 [1] NCCL INFO comm 0x5557dd826b00 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s3b0n0:24102:55014 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Setting affinity for GPU 3 to ffff
x1000c3s3b0n0:24099:55015 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000
x1000c3s3b0n0:24101:55017 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
x1000c3s3b0n0:24100:55016 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Setting affinity for GPU 1 to ffff,00000000
x1000c3s3b0n0:24102:55014 [3] NCCL INFO comm 0x560ca8e79970 rank 7 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
x1000c3s3b0n0:24101:55017 [2] NCCL INFO comm 0x55648226ad10 rank 6 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
x1000c3s3b0n0:24100:55016 [1] NCCL INFO comm 0x5557dd826b00 rank 5 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
x1000c3s3b0n0:24099:55015 [0] NCCL INFO comm 0x55ac4e7cb6a0 rank 4 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Trees [0] 5/-1/-1->7->4 [1] 4/-1/-1->7->8 [2] 5/8/-1->7->4 [3] 4/3/-1->7->15
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Trees [0] -1/-1/-1->6->5 [1] -1/-1/-1->6->5 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Trees [0] 6/-1/-1->5->7 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->7 [3] 6/-1/-1->5->4
x1000c3s3b0n0:24102:55014 [3] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Trees [0] 7/-1/-1->4->11 [1] 5/-1/-1->4->7 [2] 7/0/-1->4->12 [3] 5/11/-1->4->7
x1000c3s3b0n0:24101:55017 [2] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:24100:55016 [1] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:24099:55015 [0] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:24099:13417 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 2.
x1000c3s3b0n0:24099:13417 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
x1000c3s3b0n0:24099:13417 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:13417 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 02/0 : 3[3] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 00 : 4[0] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 02 : 4[0] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Channel 00 : 5[1] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Channel 00 : 6[2] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Channel 02 : 6[2] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Channel 02 : 5[1] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 01/0 : 4[0] -> 11[3] [send] via NET/Socket/0
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 03/0 : 4[0] -> 11[3] [send] via NET/Socket/0
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 00/0 : 7[3] -> 8[0] [send] via NET/Socket/1
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 02/0 : 7[3] -> 8[0] [send] via NET/Socket/1
x1000c3s3b0n0:24102:13415 [3] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 2.
x1000c3s3b0n0:24102:13415 [3] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
x1000c3s3b0n0:24102:13415 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 01/0 : 0[0] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24102:13415 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 03/0 : 0[0] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 01 : 7[3] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 03 : 7[3] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Channel 01 : 5[1] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Channel 03 : 5[1] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Channel 01 : 6[2] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Channel 03 : 6[2] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Connected all rings
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 01/0 : 7[3] -> 8[0] [send] via NET/Socket/0
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Connected all rings
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Connected all rings
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Connected all rings
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 01 : 4[0] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 03 : 4[0] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Channel 01 : 5[1] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Channel 03 : 5[1] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 00 : 4[0] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Channel 00 : 6[2] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Channel 00 : 5[1] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 01 : 4[0] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Channel 02 : 6[2] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Channel 02 : 5[1] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 02 : 4[0] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 03 : 4[0] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24099:13417 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:13415 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 02/0 : 0[0] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 00/0 : 4[0] -> 11[3] [send] via NET/Socket/1
x1000c3s3b0n0:24102:13415 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 03/0 : 15[3] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24099:13417 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 03/0 : 7[3] -> 15[3] [send] via NET/Socket/0
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 02/0 : 12[0] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 02/0 : 4[0] -> 12[0] [send] via NET/Socket/1
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [send] via NET/Socket/0
x1000c3s3b0n0:24099:13417 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 00/0 : 11[3] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:13417 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 03/0 : 11[3] -> 4[0] [receive] via NET/Socket/0
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 00 : 7[3] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Channel 02/0 : 4[0] -> 0[0] [send] via NET/Socket/1
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 01 : 7[3] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 02 : 7[3] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 03 : 7[3] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 00 : 7[3] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 02 : 7[3] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24102:13415 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 01/0 : 8[0] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24102:13415 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Channel 02/0 : 8[0] -> 7[3] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:55015 [0] NCCL INFO Connected all trees
x1000c3s3b0n0:24099:55015 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n0:24099:55015 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n0:24102:55014 [3] NCCL INFO Connected all trees
x1000c3s3b0n0:24101:55017 [2] NCCL INFO Connected all trees
x1000c3s3b0n0:24100:55016 [1] NCCL INFO Connected all trees
x1000c3s3b0n0:24102:55014 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n0:24101:55017 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n0:24102:55014 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n0:24100:55016 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n0:24101:55017 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n0:24100:55016 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n0:24100:55016 [1] NCCL INFO comm 0x5557dd826b00 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s3b0n0:24102:55014 [3] NCCL INFO comm 0x560ca8e79970 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s3b0n0:24099:55015 [0] NCCL INFO comm 0x55ac4e7cb6a0 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s3b0n0:24101:55017 [2] NCCL INFO comm 0x55648226ad10 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
Error opening image: /project/lt200203-aimedi/mimix-cxr/mimic-cxr-jpg/mimic-cxr-jpg/files/p13/p13140362/s52567077/1385bcfc-2c05e015-6c962ac5-6a153882-ddfe2ed1.jpg, image file is truncated (2 bytes not processed)
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Using network Socket
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Using network Socket
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Using network Socket
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Using network Socket
x1000c3s3b0n0:24102:12618 [3] NCCL INFO comm 0x560d54648ac0 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0x4eadd15c38937d75 - Init START
x1000c3s3b0n0:24100:12616 [1] NCCL INFO comm 0x55588790a9d0 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0x4eadd15c38937d75 - Init START
x1000c3s3b0n0:24101:12617 [2] NCCL INFO comm 0x5565298f3850 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0x4eadd15c38937d75 - Init START
x1000c3s3b0n0:24099:12615 [0] NCCL INFO comm 0x55acfacc6740 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0x4eadd15c38937d75 - Init START
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Setting affinity for GPU 3 to ffff
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Setting affinity for GPU 1 to ffff,00000000
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
x1000c3s3b0n0:24101:12617 [2] NCCL INFO comm 0x5565298f3850 rank 6 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
x1000c3s3b0n0:24102:12618 [3] NCCL INFO comm 0x560d54648ac0 rank 7 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Trees [0] -1/-1/-1->6->5 [1] -1/-1/-1->6->5 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
x1000c3s3b0n0:24100:12616 [1] NCCL INFO comm 0x55588790a9d0 rank 5 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
x1000c3s3b0n0:24099:12615 [0] NCCL INFO comm 0x55acfacc6740 rank 4 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Trees [0] 5/-1/-1->7->4 [1] 4/-1/-1->7->8 [2] 5/8/-1->7->4 [3] 4/3/-1->7->15
x1000c3s3b0n0:24101:12617 [2] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Trees [0] 6/-1/-1->5->7 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->7 [3] 6/-1/-1->5->4
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Trees [0] 7/-1/-1->4->11 [1] 5/-1/-1->4->7 [2] 7/0/-1->4->12 [3] 5/11/-1->4->7
x1000c3s3b0n0:24102:12618 [3] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:24100:12616 [1] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:24099:12615 [0] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:24099:12840 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:12840 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 02/0 : 3[3] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 00 : 4[0] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 02 : 4[0] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Channel 00 : 5[1] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Channel 00 : 6[2] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Channel 02 : 5[1] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Channel 02 : 6[2] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 00/0 : 7[3] -> 8[0] [send] via NET/Socket/1
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 02/0 : 7[3] -> 8[0] [send] via NET/Socket/1
x1000c3s3b0n0:24102:12839 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 01/0 : 0[0] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 01/0 : 4[0] -> 11[3] [send] via NET/Socket/0
x1000c3s3b0n0:24102:12839 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 03/0 : 0[0] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 03/0 : 4[0] -> 11[3] [send] via NET/Socket/0
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 01 : 7[3] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 03 : 7[3] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Channel 01 : 5[1] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Channel 03 : 5[1] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Channel 01 : 6[2] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Channel 03 : 6[2] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Connected all rings
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Connected all rings
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Connected all rings
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Connected all rings
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 01/0 : 7[3] -> 8[0] [send] via NET/Socket/0
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 01 : 4[0] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 03 : 4[0] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Channel 01 : 5[1] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Channel 03 : 5[1] -> 6[2] via SHM/direct/direct
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 00 : 4[0] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Channel 00 : 6[2] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Channel 00 : 5[1] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 01 : 4[0] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Channel 02 : 6[2] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Channel 02 : 5[1] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 02 : 4[0] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 03 : 4[0] -> 7[3] via SHM/direct/direct
x1000c3s3b0n0:24099:12840 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:12839 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 02/0 : 0[0] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 00/0 : 4[0] -> 11[3] [send] via NET/Socket/1
x1000c3s3b0n0:24102:12839 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 03/0 : 15[3] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 03/0 : 7[3] -> 15[3] [send] via NET/Socket/0
x1000c3s3b0n0:24099:12840 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 02/0 : 12[0] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [send] via NET/Socket/0
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 02/0 : 4[0] -> 12[0] [send] via NET/Socket/1
x1000c3s3b0n0:24099:12840 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 00/0 : 11[3] -> 4[0] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:12840 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 03/0 : 11[3] -> 4[0] [receive] via NET/Socket/0
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Channel 02/0 : 4[0] -> 0[0] [send] via NET/Socket/1
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 00 : 7[3] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 01 : 7[3] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 02 : 7[3] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 03 : 7[3] -> 4[0] via SHM/direct/direct
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 00 : 7[3] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 02 : 7[3] -> 5[1] via SHM/direct/direct
x1000c3s3b0n0:24102:12839 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 01/0 : 8[0] -> 7[3] [receive] via NET/Socket/0
x1000c3s3b0n0:24102:12839 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Channel 02/0 : 8[0] -> 7[3] [receive] via NET/Socket/1
x1000c3s3b0n0:24099:12615 [0] NCCL INFO Connected all trees
x1000c3s3b0n0:24099:12615 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n0:24099:12615 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n0:24102:12618 [3] NCCL INFO Connected all trees
x1000c3s3b0n0:24100:12616 [1] NCCL INFO Connected all trees
x1000c3s3b0n0:24101:12617 [2] NCCL INFO Connected all trees
x1000c3s3b0n0:24102:12618 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n0:24100:12616 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n0:24101:12617 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s3b0n0:24102:12618 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n0:24100:12616 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n0:24101:12617 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s3b0n0:24100:12616 [1] NCCL INFO comm 0x55588790a9d0 rank 5 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s3b0n0:24102:12618 [3] NCCL INFO comm 0x560d54648ac0 rank 7 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s3b0n0:24099:12615 [0] NCCL INFO comm 0x55acfacc6740 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s3b0n0:24101:12617 [2] NCCL INFO comm 0x5565298f3850 rank 6 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0x4eadd15c38937d75 - Init COMPLETE
Error opening image: /project/lt200203-aimedi/mimix-cxr/mimic-cxr-jpg/mimic-cxr-jpg/files/p12/p12460718/s54626053/6b966842-7a50eb2f-4d9177f3-5927b5f3-6503ef6d.jpg, cannot identify image file '/lustrefs/disk/project/lt200203-aimedi/mimix-cxr/mimic-cxr-jpg/mimic-cxr-jpg/files/p12/p12460718/s54626053/6b966842-7a50eb2f-4d9177f3-5927b5f3-6503ef6d.jpg'
Warning: No valid images found for index 54267
