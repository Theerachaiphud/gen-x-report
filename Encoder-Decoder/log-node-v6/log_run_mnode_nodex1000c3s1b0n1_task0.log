Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Lmod is automatically replacing "cce/16.0.1" with "gcc/12.2.0".


Lmod is automatically replacing "PrgEnv-cray/8.4.0" with "PrgEnv-gnu/8.4.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.27


The following have been reloaded with a version change:
  1) PrgEnv-gnu/8.4.0 => PrgEnv-gnu/8.3.3
  2) cray-libsci/23.09.1.1 => cray-libsci/23.02.1.1
  3) cray-mpich/8.1.27 => cray-mpich/8.1.25
  4) craype/2.7.23 => craype/2.7.20
  5) gcc/12.2.0 => gcc/11.2.0
  6) perftools-base/23.09.0 => perftools-base/23.03.0

SLURM enviroment
SLURM_MPI_TYPE=cray_shasta
SLURM_STEP_ID=1
SLURM_STEP_GPUS=0,1,2,3
SLURM_NODEID=0
SLURM_TASK_PID=50071
SLURM_PRIO_PROCESS=0
SLURM_CPU_BIND_VERBOSE=quiet
SLURM_SUBMIT_DIR=/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/Encoder-Decoder
SLURM_CPUS_PER_TASK=64
SLURM_STEPID=1
SLURM_SRUN_COMM_HOST=10.168.0.86
SLURM_DISTRIBUTION=cyclic
SLURM_GPUS_PER_NODE=4
SLURM_PROCID=0
SLURM_JOB_GID=1980402197
SLURM_CPU_BIND=quiet,mask_cpu:0xFFFFFFFFFFFFFFFF
SLURMD_NODENAME=lanta-g-020
SLURM_TASKS_PER_NODE=1(x4)
SLURM_NNODES=4
SLURM_LAUNCH_NODE_IPADDR=10.168.0.86
SLURM_STEP_TASKS_PER_NODE=1(x4)
SLURM_JOB_NODELIST=lanta-g-[020,023-025]
SLURM_CLUSTER_NAME=lanta
SLURM_NODELIST=lanta-g-[020,023-025]
SLURM_GPUS_ON_NODE=4
SLURM_NTASKS=4
SLURM_UMASK=0022
SLURM_JOB_CPUS_PER_NODE=64(x4)
SLURM_TOPOLOGY_ADDR=global.group2.lanta-g-020
SLURM_WORKING_CLUSTER=lanta:x3003c0s21b0n0:6817:9728:109
SLURM_STEP_NODELIST=lanta-g-[020,023-025]
SLURM_JOB_NAME=gen-x-report
SLURM_SRUN_COMM_PORT=41417
SLURM_JOB_GPUS=0,1,2,3
SLURM_JOBID=1989816
SLURM_CONF=/var/spool/slurmd/conf-cache/slurm.conf
SLURM_NODE_ALIASES=(null)
SLURM_JOB_QOS=aimedi
SLURM_TOPOLOGY_ADDR_PATTERN=switch.switch.node
SLURM_STEP_RESV_PORTS=21886-21887
SLURM_CPUS_ON_NODE=64
SLURM_JOB_NUM_NODES=4
SLURM_JOB_UID=1980402197
SLURM_JOB_PARTITION=gpu
SLURM_CPU_BIND_LIST=0xFFFFFFFFFFFFFFFF
SLURM_JOB_USER=tpornsur
SLURM_NPROCS=4
SLURM_SUBMIT_HOST=x3002c0s7b0n0
SLURM_JOB_ACCOUNT=lt200203
SLURM_STEP_LAUNCHER_PORT=41417
SLURM_GTIDS=0
SLURM_JOB_ID=1989816
SLURM_CPU_BIND_TYPE=mask_cpu:
SLURM_STEP_NUM_TASKS=4
SLURM_JOB_CPUS_PER_NODE_PACK_GROUP_0=64(x4)
SLURM_STEP_NUM_NODES=4
SLURM_LOCALID=0
Python Version:
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/bin/python
Python 3.12.5
Torch Availability:
Torch available: True
Conda Environment:                       *  /lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1
MASTER_ADDR = lanta-g-020
MASTER_PORT = 17314
num_processes 16
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[W106 00:19:54.037355967 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:54.039097885 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:54.053711012 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:54.051310027 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:54.053759244 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:54.053820081 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W106 00:19:54.055556939 Utils.hpp:164] Warning: Environment variable NCCL_BLOCKING_WAIT is deprecated; use TORCH_NCCL_BLOCKING_WAIT instead (function operator())
[W106 00:19:54.055647584 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.46it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.37it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.36it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.19it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.02it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.03it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.89it/s]
Some weights of the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b were not used when initializing Blip2ForConditionalGeneration: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'vision_model.encoder.layers.1.self_attn.projection.weight', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'vision_model.encoder.layers.10.self_attn.qkv.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'vision_model.encoder.layers.11.self_attn.projection.weight', 'vision_model.encoder.layers.11.self_attn.qkv.bias', 'vision_model.encoder.layers.11.self_attn.qkv.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.projection.bias', 'vision_model.encoder.layers.12.self_attn.projection.weight', 'vision_model.encoder.layers.12.self_attn.qkv.bias', 'vision_model.encoder.layers.12.self_attn.qkv.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.projection.bias', 'vision_model.encoder.layers.13.self_attn.projection.weight', 'vision_model.encoder.layers.13.self_attn.qkv.bias', 'vision_model.encoder.layers.13.self_attn.qkv.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.projection.bias', 'vision_model.encoder.layers.14.self_attn.projection.weight', 'vision_model.encoder.layers.14.self_attn.qkv.bias', 'vision_model.encoder.layers.14.self_attn.qkv.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.projection.bias', 'vision_model.encoder.layers.15.self_attn.projection.weight', 'vision_model.encoder.layers.15.self_attn.qkv.bias', 'vision_model.encoder.layers.15.self_attn.qkv.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.projection.bias', 'vision_model.encoder.layers.16.self_attn.projection.weight', 'vision_model.encoder.layers.16.self_attn.qkv.bias', 'vision_model.encoder.layers.16.self_attn.qkv.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.projection.bias', 'vision_model.encoder.layers.17.self_attn.projection.weight', 'vision_model.encoder.layers.17.self_attn.qkv.bias', 'vision_model.encoder.layers.17.self_attn.qkv.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.projection.bias', 'vision_model.encoder.layers.18.self_attn.projection.weight', 'vision_model.encoder.layers.18.self_attn.qkv.bias', 'vision_model.encoder.layers.18.self_attn.qkv.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.projection.bias', 'vision_model.encoder.layers.19.self_attn.projection.weight', 'vision_model.encoder.layers.19.self_attn.qkv.bias', 'vision_model.encoder.layers.19.self_attn.qkv.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.projection.bias', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.projection.bias', 'vision_model.encoder.layers.20.self_attn.projection.weight', 'vision_model.encoder.layers.20.self_attn.qkv.bias', 'vision_model.encoder.layers.20.self_attn.qkv.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.projection.bias', 'vision_model.encoder.layers.21.self_attn.projection.weight', 'vision_model.encoder.layers.21.self_attn.qkv.bias', 'vision_model.encoder.layers.21.self_attn.qkv.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.projection.bias', 'vision_model.encoder.layers.22.self_attn.projection.weight', 'vision_model.encoder.layers.22.self_attn.qkv.bias', 'vision_model.encoder.layers.22.self_attn.qkv.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.projection.bias', 'vision_model.encoder.layers.23.self_attn.projection.weight', 'vision_model.encoder.layers.23.self_attn.qkv.bias', 'vision_model.encoder.layers.23.self_attn.qkv.weight', 'vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.self_attn.projection.bias', 'vision_model.encoder.layers.24.self_attn.projection.weight', 'vision_model.encoder.layers.24.self_attn.qkv.bias', 'vision_model.encoder.layers.24.self_attn.qkv.weight', 'vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.self_attn.projection.bias', 'vision_model.encoder.layers.25.self_attn.projection.weight', 'vision_model.encoder.layers.25.self_attn.qkv.bias', 'vision_model.encoder.layers.25.self_attn.qkv.weight', 'vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.self_attn.projection.bias', 'vision_model.encoder.layers.26.self_attn.projection.weight', 'vision_model.encoder.layers.26.self_attn.qkv.bias', 'vision_model.encoder.layers.26.self_attn.qkv.weight', 'vision_model.encoder.layers.27.layer_norm1.bias', 'vision_model.encoder.layers.27.layer_norm1.weight', 'vision_model.encoder.layers.27.layer_norm2.bias', 'vision_model.encoder.layers.27.layer_norm2.weight', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.self_attn.projection.bias', 'vision_model.encoder.layers.27.self_attn.projection.weight', 'vision_model.encoder.layers.27.self_attn.qkv.bias', 'vision_model.encoder.layers.27.self_attn.qkv.weight', 'vision_model.encoder.layers.28.layer_norm1.bias', 'vision_model.encoder.layers.28.layer_norm1.weight', 'vision_model.encoder.layers.28.layer_norm2.bias', 'vision_model.encoder.layers.28.layer_norm2.weight', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.self_attn.projection.bias', 'vision_model.encoder.layers.28.self_attn.projection.weight', 'vision_model.encoder.layers.28.self_attn.qkv.bias', 'vision_model.encoder.layers.28.self_attn.qkv.weight', 'vision_model.encoder.layers.29.layer_norm1.bias', 'vision_model.encoder.layers.29.layer_norm1.weight', 'vision_model.encoder.layers.29.layer_norm2.bias', 'vision_model.encoder.layers.29.layer_norm2.weight', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.self_attn.projection.bias', 'vision_model.encoder.layers.29.self_attn.projection.weight', 'vision_model.encoder.layers.29.self_attn.qkv.bias', 'vision_model.encoder.layers.29.self_attn.qkv.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.projection.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'vision_model.encoder.layers.30.layer_norm1.bias', 'vision_model.encoder.layers.30.layer_norm1.weight', 'vision_model.encoder.layers.30.layer_norm2.bias', 'vision_model.encoder.layers.30.layer_norm2.weight', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.self_attn.projection.bias', 'vision_model.encoder.layers.30.self_attn.projection.weight', 'vision_model.encoder.layers.30.self_attn.qkv.bias', 'vision_model.encoder.layers.30.self_attn.qkv.weight', 'vision_model.encoder.layers.31.layer_norm1.bias', 'vision_model.encoder.layers.31.layer_norm1.weight', 'vision_model.encoder.layers.31.layer_norm2.bias', 'vision_model.encoder.layers.31.layer_norm2.weight', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.self_attn.projection.bias', 'vision_model.encoder.layers.31.self_attn.projection.weight', 'vision_model.encoder.layers.31.self_attn.qkv.bias', 'vision_model.encoder.layers.31.self_attn.qkv.weight', 'vision_model.encoder.layers.32.layer_norm1.bias', 'vision_model.encoder.layers.32.layer_norm1.weight', 'vision_model.encoder.layers.32.layer_norm2.bias', 'vision_model.encoder.layers.32.layer_norm2.weight', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.self_attn.projection.bias', 'vision_model.encoder.layers.32.self_attn.projection.weight', 'vision_model.encoder.layers.32.self_attn.qkv.bias', 'vision_model.encoder.layers.32.self_attn.qkv.weight', 'vision_model.encoder.layers.33.layer_norm1.bias', 'vision_model.encoder.layers.33.layer_norm1.weight', 'vision_model.encoder.layers.33.layer_norm2.bias', 'vision_model.encoder.layers.33.layer_norm2.weight', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.self_attn.projection.bias', 'vision_model.encoder.layers.33.self_attn.projection.weight', 'vision_model.encoder.layers.33.self_attn.qkv.bias', 'vision_model.encoder.layers.33.self_attn.qkv.weight', 'vision_model.encoder.layers.34.layer_norm1.bias', 'vision_model.encoder.layers.34.layer_norm1.weight', 'vision_model.encoder.layers.34.layer_norm2.bias', 'vision_model.encoder.layers.34.layer_norm2.weight', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.self_attn.projection.bias', 'vision_model.encoder.layers.34.self_attn.projection.weight', 'vision_model.encoder.layers.34.self_attn.qkv.bias', 'vision_model.encoder.layers.34.self_attn.qkv.weight', 'vision_model.encoder.layers.35.layer_norm1.bias', 'vision_model.encoder.layers.35.layer_norm1.weight', 'vision_model.encoder.layers.35.layer_norm2.bias', 'vision_model.encoder.layers.35.layer_norm2.weight', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.self_attn.projection.bias', 'vision_model.encoder.layers.35.self_attn.projection.weight', 'vision_model.encoder.layers.35.self_attn.qkv.bias', 'vision_model.encoder.layers.35.self_attn.qkv.weight', 'vision_model.encoder.layers.36.layer_norm1.bias', 'vision_model.encoder.layers.36.layer_norm1.weight', 'vision_model.encoder.layers.36.layer_norm2.bias', 'vision_model.encoder.layers.36.layer_norm2.weight', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.self_attn.projection.bias', 'vision_model.encoder.layers.36.self_attn.projection.weight', 'vision_model.encoder.layers.36.self_attn.qkv.bias', 'vision_model.encoder.layers.36.self_attn.qkv.weight', 'vision_model.encoder.layers.37.layer_norm1.bias', 'vision_model.encoder.layers.37.layer_norm1.weight', 'vision_model.encoder.layers.37.layer_norm2.bias', 'vision_model.encoder.layers.37.layer_norm2.weight', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.self_attn.projection.bias', 'vision_model.encoder.layers.37.self_attn.projection.weight', 'vision_model.encoder.layers.37.self_attn.qkv.bias', 'vision_model.encoder.layers.37.self_attn.qkv.weight', 'vision_model.encoder.layers.38.layer_norm1.bias', 'vision_model.encoder.layers.38.layer_norm1.weight', 'vision_model.encoder.layers.38.layer_norm2.bias', 'vision_model.encoder.layers.38.layer_norm2.weight', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.self_attn.projection.bias', 'vision_model.encoder.layers.38.self_attn.projection.weight', 'vision_model.encoder.layers.38.self_attn.qkv.bias', 'vision_model.encoder.layers.38.self_attn.qkv.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.projection.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'vision_model.encoder.layers.5.self_attn.projection.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'vision_model.encoder.layers.5.self_attn.qkv.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.6.self_attn.qkv.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'vision_model.encoder.layers.8.self_attn.qkv.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight']
- This IS expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Blip2ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized: ['vision_model.embeddings.cls_token', 'vision_model.embeddings.mask_token', 'vision_model.embeddings.patch_embeddings.projection.bias', 'vision_model.embeddings.patch_embeddings.projection.weight', 'vision_model.embeddings.position_embeddings', 'vision_model.encoder.layer.0.attention.attention.key.bias', 'vision_model.encoder.layer.0.attention.attention.key.weight', 'vision_model.encoder.layer.0.attention.attention.query.bias', 'vision_model.encoder.layer.0.attention.attention.query.weight', 'vision_model.encoder.layer.0.attention.attention.value.bias', 'vision_model.encoder.layer.0.attention.attention.value.weight', 'vision_model.encoder.layer.0.attention.output.dense.bias', 'vision_model.encoder.layer.0.attention.output.dense.weight', 'vision_model.encoder.layer.0.layer_scale1.lambda1', 'vision_model.encoder.layer.0.layer_scale2.lambda1', 'vision_model.encoder.layer.0.mlp.fc1.bias', 'vision_model.encoder.layer.0.mlp.fc1.weight', 'vision_model.encoder.layer.0.mlp.fc2.bias', 'vision_model.encoder.layer.0.mlp.fc2.weight', 'vision_model.encoder.layer.0.norm1.bias', 'vision_model.encoder.layer.0.norm1.weight', 'vision_model.encoder.layer.0.norm2.bias', 'vision_model.encoder.layer.0.norm2.weight', 'vision_model.encoder.layer.1.attention.attention.key.bias', 'vision_model.encoder.layer.1.attention.attention.key.weight', 'vision_model.encoder.layer.1.attention.attention.query.bias', 'vision_model.encoder.layer.1.attention.attention.query.weight', 'vision_model.encoder.layer.1.attention.attention.value.bias', 'vision_model.encoder.layer.1.attention.attention.value.weight', 'vision_model.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layer.1.attention.output.dense.weight', 'vision_model.encoder.layer.1.layer_scale1.lambda1', 'vision_model.encoder.layer.1.layer_scale2.lambda1', 'vision_model.encoder.layer.1.mlp.fc1.bias', 'vision_model.encoder.layer.1.mlp.fc1.weight', 'vision_model.encoder.layer.1.mlp.fc2.bias', 'vision_model.encoder.layer.1.mlp.fc2.weight', 'vision_model.encoder.layer.1.norm1.bias', 'vision_model.encoder.layer.1.norm1.weight', 'vision_model.encoder.layer.1.norm2.bias', 'vision_model.encoder.layer.1.norm2.weight', 'vision_model.encoder.layer.10.attention.attention.key.bias', 'vision_model.encoder.layer.10.attention.attention.key.weight', 'vision_model.encoder.layer.10.attention.attention.query.bias', 'vision_model.encoder.layer.10.attention.attention.query.weight', 'vision_model.encoder.layer.10.attention.attention.value.bias', 'vision_model.encoder.layer.10.attention.attention.value.weight', 'vision_model.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layer.10.attention.output.dense.weight', 'vision_model.encoder.layer.10.layer_scale1.lambda1', 'vision_model.encoder.layer.10.layer_scale2.lambda1', 'vision_model.encoder.layer.10.mlp.fc1.bias', 'vision_model.encoder.layer.10.mlp.fc1.weight', 'vision_model.encoder.layer.10.mlp.fc2.bias', 'vision_model.encoder.layer.10.mlp.fc2.weight', 'vision_model.encoder.layer.10.norm1.bias', 'vision_model.encoder.layer.10.norm1.weight', 'vision_model.encoder.layer.10.norm2.bias', 'vision_model.encoder.layer.10.norm2.weight', 'vision_model.encoder.layer.11.attention.attention.key.bias', 'vision_model.encoder.layer.11.attention.attention.key.weight', 'vision_model.encoder.layer.11.attention.attention.query.bias', 'vision_model.encoder.layer.11.attention.attention.query.weight', 'vision_model.encoder.layer.11.attention.attention.value.bias', 'vision_model.encoder.layer.11.attention.attention.value.weight', 'vision_model.encoder.layer.11.attention.output.dense.bias', 'vision_model.encoder.layer.11.attention.output.dense.weight', 'vision_model.encoder.layer.11.layer_scale1.lambda1', 'vision_model.encoder.layer.11.layer_scale2.lambda1', 'vision_model.encoder.layer.11.mlp.fc1.bias', 'vision_model.encoder.layer.11.mlp.fc1.weight', 'vision_model.encoder.layer.11.mlp.fc2.bias', 'vision_model.encoder.layer.11.mlp.fc2.weight', 'vision_model.encoder.layer.11.norm1.bias', 'vision_model.encoder.layer.11.norm1.weight', 'vision_model.encoder.layer.11.norm2.bias', 'vision_model.encoder.layer.11.norm2.weight', 'vision_model.encoder.layer.2.attention.attention.key.bias', 'vision_model.encoder.layer.2.attention.attention.key.weight', 'vision_model.encoder.layer.2.attention.attention.query.bias', 'vision_model.encoder.layer.2.attention.attention.query.weight', 'vision_model.encoder.layer.2.attention.attention.value.bias', 'vision_model.encoder.layer.2.attention.attention.value.weight', 'vision_model.encoder.layer.2.attention.output.dense.bias', 'vision_model.encoder.layer.2.attention.output.dense.weight', 'vision_model.encoder.layer.2.layer_scale1.lambda1', 'vision_model.encoder.layer.2.layer_scale2.lambda1', 'vision_model.encoder.layer.2.mlp.fc1.bias', 'vision_model.encoder.layer.2.mlp.fc1.weight', 'vision_model.encoder.layer.2.mlp.fc2.bias', 'vision_model.encoder.layer.2.mlp.fc2.weight', 'vision_model.encoder.layer.2.norm1.bias', 'vision_model.encoder.layer.2.norm1.weight', 'vision_model.encoder.layer.2.norm2.bias', 'vision_model.encoder.layer.2.norm2.weight', 'vision_model.encoder.layer.3.attention.attention.key.bias', 'vision_model.encoder.layer.3.attention.attention.key.weight', 'vision_model.encoder.layer.3.attention.attention.query.bias', 'vision_model.encoder.layer.3.attention.attention.query.weight', 'vision_model.encoder.layer.3.attention.attention.value.bias', 'vision_model.encoder.layer.3.attention.attention.value.weight', 'vision_model.encoder.layer.3.attention.output.dense.bias', 'vision_model.encoder.layer.3.attention.output.dense.weight', 'vision_model.encoder.layer.3.layer_scale1.lambda1', 'vision_model.encoder.layer.3.layer_scale2.lambda1', 'vision_model.encoder.layer.3.mlp.fc1.bias', 'vision_model.encoder.layer.3.mlp.fc1.weight', 'vision_model.encoder.layer.3.mlp.fc2.bias', 'vision_model.encoder.layer.3.mlp.fc2.weight', 'vision_model.encoder.layer.3.norm1.bias', 'vision_model.encoder.layer.3.norm1.weight', 'vision_model.encoder.layer.3.norm2.bias', 'vision_model.encoder.layer.3.norm2.weight', 'vision_model.encoder.layer.4.attention.attention.key.bias', 'vision_model.encoder.layer.4.attention.attention.key.weight', 'vision_model.encoder.layer.4.attention.attention.query.bias', 'vision_model.encoder.layer.4.attention.attention.query.weight', 'vision_model.encoder.layer.4.attention.attention.value.bias', 'vision_model.encoder.layer.4.attention.attention.value.weight', 'vision_model.encoder.layer.4.attention.output.dense.bias', 'vision_model.encoder.layer.4.attention.output.dense.weight', 'vision_model.encoder.layer.4.layer_scale1.lambda1', 'vision_model.encoder.layer.4.layer_scale2.lambda1', 'vision_model.encoder.layer.4.mlp.fc1.bias', 'vision_model.encoder.layer.4.mlp.fc1.weight', 'vision_model.encoder.layer.4.mlp.fc2.bias', 'vision_model.encoder.layer.4.mlp.fc2.weight', 'vision_model.encoder.layer.4.norm1.bias', 'vision_model.encoder.layer.4.norm1.weight', 'vision_model.encoder.layer.4.norm2.bias', 'vision_model.encoder.layer.4.norm2.weight', 'vision_model.encoder.layer.5.attention.attention.key.bias', 'vision_model.encoder.layer.5.attention.attention.key.weight', 'vision_model.encoder.layer.5.attention.attention.query.bias', 'vision_model.encoder.layer.5.attention.attention.query.weight', 'vision_model.encoder.layer.5.attention.attention.value.bias', 'vision_model.encoder.layer.5.attention.attention.value.weight', 'vision_model.encoder.layer.5.attention.output.dense.bias', 'vision_model.encoder.layer.5.attention.output.dense.weight', 'vision_model.encoder.layer.5.layer_scale1.lambda1', 'vision_model.encoder.layer.5.layer_scale2.lambda1', 'vision_model.encoder.layer.5.mlp.fc1.bias', 'vision_model.encoder.layer.5.mlp.fc1.weight', 'vision_model.encoder.layer.5.mlp.fc2.bias', 'vision_model.encoder.layer.5.mlp.fc2.weight', 'vision_model.encoder.layer.5.norm1.bias', 'vision_model.encoder.layer.5.norm1.weight', 'vision_model.encoder.layer.5.norm2.bias', 'vision_model.encoder.layer.5.norm2.weight', 'vision_model.encoder.layer.6.attention.attention.key.bias', 'vision_model.encoder.layer.6.attention.attention.key.weight', 'vision_model.encoder.layer.6.attention.attention.query.bias', 'vision_model.encoder.layer.6.attention.attention.query.weight', 'vision_model.encoder.layer.6.attention.attention.value.bias', 'vision_model.encoder.layer.6.attention.attention.value.weight', 'vision_model.encoder.layer.6.attention.output.dense.bias', 'vision_model.encoder.layer.6.attention.output.dense.weight', 'vision_model.encoder.layer.6.layer_scale1.lambda1', 'vision_model.encoder.layer.6.layer_scale2.lambda1', 'vision_model.encoder.layer.6.mlp.fc1.bias', 'vision_model.encoder.layer.6.mlp.fc1.weight', 'vision_model.encoder.layer.6.mlp.fc2.bias', 'vision_model.encoder.layer.6.mlp.fc2.weight', 'vision_model.encoder.layer.6.norm1.bias', 'vision_model.encoder.layer.6.norm1.weight', 'vision_model.encoder.layer.6.norm2.bias', 'vision_model.encoder.layer.6.norm2.weight', 'vision_model.encoder.layer.7.attention.attention.key.bias', 'vision_model.encoder.layer.7.attention.attention.key.weight', 'vision_model.encoder.layer.7.attention.attention.query.bias', 'vision_model.encoder.layer.7.attention.attention.query.weight', 'vision_model.encoder.layer.7.attention.attention.value.bias', 'vision_model.encoder.layer.7.attention.attention.value.weight', 'vision_model.encoder.layer.7.attention.output.dense.bias', 'vision_model.encoder.layer.7.attention.output.dense.weight', 'vision_model.encoder.layer.7.layer_scale1.lambda1', 'vision_model.encoder.layer.7.layer_scale2.lambda1', 'vision_model.encoder.layer.7.mlp.fc1.bias', 'vision_model.encoder.layer.7.mlp.fc1.weight', 'vision_model.encoder.layer.7.mlp.fc2.bias', 'vision_model.encoder.layer.7.mlp.fc2.weight', 'vision_model.encoder.layer.7.norm1.bias', 'vision_model.encoder.layer.7.norm1.weight', 'vision_model.encoder.layer.7.norm2.bias', 'vision_model.encoder.layer.7.norm2.weight', 'vision_model.encoder.layer.8.attention.attention.key.bias', 'vision_model.encoder.layer.8.attention.attention.key.weight', 'vision_model.encoder.layer.8.attention.attention.query.bias', 'vision_model.encoder.layer.8.attention.attention.query.weight', 'vision_model.encoder.layer.8.attention.attention.value.bias', 'vision_model.encoder.layer.8.attention.attention.value.weight', 'vision_model.encoder.layer.8.attention.output.dense.bias', 'vision_model.encoder.layer.8.attention.output.dense.weight', 'vision_model.encoder.layer.8.layer_scale1.lambda1', 'vision_model.encoder.layer.8.layer_scale2.lambda1', 'vision_model.encoder.layer.8.mlp.fc1.bias', 'vision_model.encoder.layer.8.mlp.fc1.weight', 'vision_model.encoder.layer.8.mlp.fc2.bias', 'vision_model.encoder.layer.8.mlp.fc2.weight', 'vision_model.encoder.layer.8.norm1.bias', 'vision_model.encoder.layer.8.norm1.weight', 'vision_model.encoder.layer.8.norm2.bias', 'vision_model.encoder.layer.8.norm2.weight', 'vision_model.encoder.layer.9.attention.attention.key.bias', 'vision_model.encoder.layer.9.attention.attention.key.weight', 'vision_model.encoder.layer.9.attention.attention.query.bias', 'vision_model.encoder.layer.9.attention.attention.query.weight', 'vision_model.encoder.layer.9.attention.attention.value.bias', 'vision_model.encoder.layer.9.attention.attention.value.weight', 'vision_model.encoder.layer.9.attention.output.dense.bias', 'vision_model.encoder.layer.9.attention.output.dense.weight', 'vision_model.encoder.layer.9.layer_scale1.lambda1', 'vision_model.encoder.layer.9.layer_scale2.lambda1', 'vision_model.encoder.layer.9.mlp.fc1.bias', 'vision_model.encoder.layer.9.mlp.fc1.weight', 'vision_model.encoder.layer.9.mlp.fc2.bias', 'vision_model.encoder.layer.9.mlp.fc2.weight', 'vision_model.encoder.layer.9.norm1.bias', 'vision_model.encoder.layer.9.norm1.weight', 'vision_model.encoder.layer.9.norm2.bias', 'vision_model.encoder.layer.9.norm2.weight', 'vision_model.layernorm.bias', 'vision_model.layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at /project/lt200203-aimedi/pud/gen-x-report/model/blip2-opt-2.7b and are newly initialized because the shapes did not match:
- qformer.encoder.layer.0.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.0.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.10.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.2.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.4.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.6.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.key.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
- qformer.encoder.layer.8.crossattention.attention.value.weight: found shape torch.Size([768, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00, 11.30it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00, 11.20it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00, 11.05it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00, 11.11it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00, 11.96it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00, 11.91it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00, 11.81it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00, 11.75it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.31it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.14it/s]
Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.21it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.05it/s]
Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.18it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.99it/s]
Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 12.10it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 11.92it/s]
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-01-06 00:20:25,547] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:25,555] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-01-06 00:20:25,618] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
train: 65720
eval: 16431
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-01-06 00:20:25,690] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 00:20:26,654] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-06 00:20:26,654] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-06 00:20:26,654] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-06 00:20:26,673] [INFO] [comm.py:652:init_distributed] cdb=None
x1000c3s1b0n1:59117:59117 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59117:59117 [0] NCCL INFO Bootstrap : Using hsn0:10.150.1.66<0>
x1000c3s1b0n1:59117:59117 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s1b0n1:59118:59118 [1] NCCL INFO cudaDriverVersion 12000
x1000c3s1b0n1:59118:59118 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59118:59118 [1] NCCL INFO Bootstrap : Using hsn0:10.150.1.66<0>
x1000c3s1b0n1:59118:59118 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s1b0n1:59120:59120 [3] NCCL INFO cudaDriverVersion 12000
x1000c3s1b0n1:59120:59120 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59119:59119 [2] NCCL INFO cudaDriverVersion 12000
x1000c3s1b0n1:59119:59119 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59120:59120 [3] NCCL INFO Bootstrap : Using hsn0:10.150.1.66<0>
x1000c3s1b0n1:59119:59119 [2] NCCL INFO Bootstrap : Using hsn0:10.150.1.66<0>
x1000c3s1b0n1:59120:59120 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s1b0n1:59119:59119 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c3s1b0n1:59117:59117 [0] NCCL INFO cudaDriverVersion 12000
NCCL version 2.20.5+cuda11.0
x1000c3s1b0n1:59120:24939 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59118:24941 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59117:24938 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59119:24940 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59120:24939 [3] NCCL INFO NET/IB : No device found.
x1000c3s1b0n1:59118:24941 [1] NCCL INFO NET/IB : No device found.
x1000c3s1b0n1:59119:24940 [2] NCCL INFO NET/IB : No device found.
x1000c3s1b0n1:59117:24938 [0] NCCL INFO NET/IB : No device found.
x1000c3s1b0n1:59120:24939 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59118:24941 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59119:24940 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59117:24938 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
x1000c3s1b0n1:59120:24939 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.66<0> [1]hsn1:10.150.1.62<0>
x1000c3s1b0n1:59118:24941 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.66<0> [1]hsn1:10.150.1.62<0>
x1000c3s1b0n1:59117:24938 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.66<0> [1]hsn1:10.150.1.62<0>
x1000c3s1b0n1:59119:24940 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.66<0> [1]hsn1:10.150.1.62<0>
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Using non-device net plugin version 0
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Using non-device net plugin version 0
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Using non-device net plugin version 0
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Using network Socket
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Using non-device net plugin version 0
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Using network Socket
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Using network Socket
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Using network Socket
x1000c3s1b0n1:59120:24939 [3] NCCL INFO comm 0x564259d31f80 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s1b0n1:59119:24940 [2] NCCL INFO comm 0x55c1c5862d80 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s1b0n1:59117:24938 [0] NCCL INFO comm 0x55aaf85e5720 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s1b0n1:59118:24941 [1] NCCL INFO comm 0x5607b3a89860 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0xdf3ce34961cb2ef2 - Init START
x1000c3s1b0n1:59120:24939 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Setting affinity for GPU 3 to ffff
x1000c3s1b0n1:59119:24940 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
x1000c3s1b0n1:59118:24941 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Setting affinity for GPU 1 to ffff,00000000
x1000c3s1b0n1:59117:24938 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000
x1000c3s1b0n1:59120:24939 [3] NCCL INFO comm 0x564259d31f80 rank 3 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
x1000c3s1b0n1:59118:24941 [1] NCCL INFO comm 0x5607b3a89860 rank 1 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
x1000c3s1b0n1:59117:24938 [0] NCCL INFO comm 0x55aaf85e5720 rank 0 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Trees [0] 2/-1/-1->1->3 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->3 [3] 2/-1/-1->1->0
x1000c3s1b0n1:59119:24940 [2] NCCL INFO comm 0x55c1c5862d80 rank 2 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Trees [0] 1/-1/-1->3->0 [1] 0/11/-1->3->-1 [2] 1/-1/-1->3->0 [3] 0/-1/-1->3->7
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
x1000c3s1b0n1:59118:24941 [1] NCCL INFO P2P Chunksize set to 131072
x1000c3s1b0n1:59120:24939 [3] NCCL INFO P2P Chunksize set to 131072
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Trees [0] -1/-1/-1->2->1 [1] -1/-1/-1->2->1 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 01/04 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
x1000c3s1b0n1:59119:24940 [2] NCCL INFO P2P Chunksize set to 131072
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 03/04 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Trees [0] 3/8/-1->0->-1 [1] 1/-1/-1->0->3 [2] 3/-1/-1->0->4 [3] 1/-1/-1->0->3
x1000c3s1b0n1:59117:24938 [0] NCCL INFO P2P Chunksize set to 131072
x1000c3s1b0n1:59117:48552 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 2.
x1000c3s1b0n1:59117:48552 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
x1000c3s1b0n1:59117:48552 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 00/0 : 15[3] -> 0[0] [receive] via NET/Socket/1
x1000c3s1b0n1:59117:48552 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 02/0 : 15[3] -> 0[0] [receive] via NET/Socket/1
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 02 : 0[0] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Channel 02 : 2[2] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Channel 02 : 1[1] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[3] [send] via NET/Socket/0
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[3] [send] via NET/Socket/0
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [send] via NET/Socket/1
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[0] [send] via NET/Socket/1
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Channel 03 : 1[1] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Channel 03 : 2[2] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59120:48554 [3] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 2.
x1000c3s1b0n1:59120:48554 [3] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
x1000c3s1b0n1:59120:48554 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 01/0 : 12[0] -> 3[3] [receive] via NET/Socket/0
x1000c3s1b0n1:59120:48554 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 03/0 : 12[0] -> 3[3] [receive] via NET/Socket/0
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 03 : 3[3] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Connected all rings
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Connected all rings
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 03 : 0[0] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Connected all rings
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Connected all rings
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Channel 03 : 1[1] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 00 : 0[0] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Channel 00 : 1[1] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 01 : 0[0] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Channel 02 : 2[2] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Channel 02 : 1[1] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 02 : 0[0] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 03 : 0[0] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 02/0 : 0[0] -> 4[0] [send] via NET/Socket/1
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [send] via NET/Socket/0
x1000c3s1b0n1:59117:48552 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/Socket/1
x1000c3s1b0n1:59120:48554 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [receive] via NET/Socket/0
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/Socket/1
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [send] via NET/Socket/0
x1000c3s1b0n1:59117:48552 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Channel 02/0 : 4[0] -> 0[0] [receive] via NET/Socket/1
x1000c3s1b0n1:59120:48554 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [receive] via NET/Socket/0
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 02 : 3[3] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 03 : 3[3] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 00 : 3[3] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Channel 02 : 3[3] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59117:24938 [0] NCCL INFO Connected all trees
x1000c3s1b0n1:59120:24939 [3] NCCL INFO Connected all trees
x1000c3s1b0n1:59117:24938 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s1b0n1:59117:24938 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s1b0n1:59120:24939 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s1b0n1:59119:24940 [2] NCCL INFO Connected all trees
x1000c3s1b0n1:59120:24939 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s1b0n1:59118:24941 [1] NCCL INFO Connected all trees
x1000c3s1b0n1:59119:24940 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s1b0n1:59119:24940 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s1b0n1:59118:24941 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s1b0n1:59118:24941 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s1b0n1:59117:24938 [0] NCCL INFO comm 0x55aaf85e5720 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s1b0n1:59120:24939 [3] NCCL INFO comm 0x564259d31f80 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s1b0n1:59119:24940 [2] NCCL INFO comm 0x55c1c5862d80 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
x1000c3s1b0n1:59118:24941 [1] NCCL INFO comm 0x5607b3a89860 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0xdf3ce34961cb2ef2 - Init COMPLETE
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.18.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|          | 0/2570 [00:00<?, ?it/s]/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/lustrefs/disk/project/lt200203-aimedi/pud/gen-x-report/env-1/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  0%|          | 1/2570 [00:22<16:10:48, 22.67s/it]  0%|          | 2/2570 [00:27<8:31:32, 11.95s/it]   0%|          | 3/2570 [00:31<6:08:30,  8.61s/it]  0%|          | 4/2570 [00:36<5:00:15,  7.02s/it]  0%|          | 5/2570 [00:40<4:21:21,  6.11s/it]  0%|          | 6/2570 [00:45<4:02:28,  5.67s/it]  0%|          | 7/2570 [00:50<3:49:07,  5.36s/it]  0%|          | 8/2570 [00:54<3:37:32,  5.09s/it]  0%|          | 9/2570 [00:59<3:30:37,  4.93s/it]  0%|          | 10/2570 [01:03<3:22:00,  4.73s/it]  0%|          | 11/2570 [01:08<3:25:52,  4.83s/it]  0%|          | 12/2570 [01:13<3:22:36,  4.75s/it]  1%|          | 13/2570 [01:17<3:18:32,  4.66s/it]  1%|          | 14/2570 [01:22<3:17:21,  4.63s/it]  1%|          | 15/2570 [01:27<3:16:54,  4.62s/it]  1%|          | 16/2570 [01:31<3:16:59,  4.63s/it]  1%|          | 17/2570 [01:36<3:14:01,  4.56s/it]  1%|          | 18/2570 [01:41<3:22:41,  4.77s/it]  1%|          | 19/2570 [01:45<3:18:49,  4.68s/it]  1%|          | 20/2570 [01:50<3:17:20,  4.64s/it]  1%|          | 21/2570 [01:54<3:16:48,  4.63s/it]  1%|          | 22/2570 [01:59<3:19:13,  4.69s/it]  1%|          | 23/2570 [02:04<3:17:52,  4.66s/it]  1%|          | 24/2570 [02:09<3:17:31,  4.66s/it]  1%|          | 25/2570 [02:13<3:15:05,  4.60s/it]  1%|          | 26/2570 [02:18<3:16:42,  4.64s/it]  1%|          | 27/2570 [02:22<3:15:01,  4.60s/it]  1%|          | 28/2570 [02:27<3:11:22,  4.52s/it]  1%|          | 29/2570 [02:31<3:15:08,  4.61s/it]  1%|          | 30/2570 [02:36<3:17:58,  4.68s/it]  1%|          | 31/2570 [02:41<3:15:06,  4.61s/it]  1%|          | 32/2570 [02:46<3:19:48,  4.72s/it]  1%|▏         | 33/2570 [02:50<3:18:30,  4.69s/it]  1%|▏         | 34/2570 [02:55<3:22:04,  4.78s/it]  1%|▏         | 35/2570 [03:00<3:27:53,  4.92s/it]  1%|▏         | 36/2570 [03:05<3:22:50,  4.80s/it]  1%|▏         | 37/2570 [03:10<3:21:40,  4.78s/it]  1%|▏         | 38/2570 [03:14<3:18:22,  4.70s/it]  2%|▏         | 39/2570 [03:19<3:15:34,  4.64s/it]  2%|▏         | 40/2570 [03:23<3:12:54,  4.57s/it]  2%|▏         | 41/2570 [03:28<3:13:48,  4.60s/it]  2%|▏         | 42/2570 [03:33<3:17:14,  4.68s/it]  2%|▏         | 43/2570 [03:38<3:21:34,  4.79s/it]  2%|▏         | 44/2570 [03:42<3:19:45,  4.74s/it]  2%|▏         | 45/2570 [03:47<3:19:25,  4.74s/it]  2%|▏         | 46/2570 [03:52<3:16:28,  4.67s/it]  2%|▏         | 47/2570 [03:56<3:16:24,  4.67s/it]  2%|▏         | 48/2570 [04:01<3:15:42,  4.66s/it]  2%|▏         | 49/2570 [04:06<3:15:46,  4.66s/it]  2%|▏         | 50/2570 [04:10<3:14:59,  4.64s/it]  2%|▏         | 51/2570 [04:15<3:15:30,  4.66s/it]  2%|▏         | 52/2570 [04:20<3:22:33,  4.83s/it]  2%|▏         | 53/2570 [04:25<3:18:10,  4.72s/it]  2%|▏         | 54/2570 [04:29<3:14:26,  4.64s/it]  2%|▏         | 55/2570 [04:34<3:16:34,  4.69s/it]  2%|▏         | 56/2570 [04:39<3:19:08,  4.75s/it]  2%|▏         | 57/2570 [04:43<3:15:20,  4.66s/it]  2%|▏         | 58/2570 [04:48<3:17:45,  4.72s/it]  2%|▏         | 59/2570 [04:53<3:24:39,  4.89s/it]  2%|▏         | 60/2570 [04:58<3:19:39,  4.77s/it]  2%|▏         | 61/2570 [05:02<3:17:39,  4.73s/it]  2%|▏         | 62/2570 [05:07<3:13:22,  4.63s/it]  2%|▏         | 63/2570 [05:11<3:10:57,  4.57s/it]  2%|▏         | 64/2570 [05:16<3:07:28,  4.49s/it]  3%|▎         | 65/2570 [05:20<3:12:01,  4.60s/it]  3%|▎         | 66/2570 [05:25<3:17:30,  4.73s/it]  3%|▎         | 67/2570 [05:30<3:12:38,  4.62s/it]  3%|▎         | 68/2570 [05:34<3:11:16,  4.59s/it]  3%|▎         | 69/2570 [05:39<3:18:13,  4.76s/it]  3%|▎         | 70/2570 [05:44<3:16:50,  4.72s/it]  3%|▎         | 71/2570 [05:49<3:19:02,  4.78s/it]  3%|▎         | 72/2570 [05:54<3:22:04,  4.85s/it]  3%|▎         | 73/2570 [05:59<3:18:15,  4.76s/it]  3%|▎         | 74/2570 [06:03<3:16:22,  4.72s/it]  3%|▎         | 75/2570 [06:08<3:21:19,  4.84s/it]  3%|▎         | 76/2570 [06:13<3:21:01,  4.84s/it]  3%|▎         | 77/2570 [06:18<3:16:39,  4.73s/it]  3%|▎         | 78/2570 [06:22<3:16:45,  4.74s/it]  3%|▎         | 79/2570 [06:28<3:21:57,  4.86s/it]  3%|▎         | 80/2570 [06:33<3:26:58,  4.99s/it]  3%|▎         | 81/2570 [06:38<3:31:59,  5.11s/it]  3%|▎         | 82/2570 [06:43<3:27:04,  4.99s/it]  3%|▎         | 83/2570 [06:47<3:20:45,  4.84s/it]  3%|▎         | 84/2570 [06:52<3:20:57,  4.85s/it]  3%|▎         | 85/2570 [06:57<3:19:25,  4.82s/it]  3%|▎         | 86/2570 [07:02<3:17:07,  4.76s/it]  3%|▎         | 87/2570 [07:06<3:15:43,  4.73s/it]  3%|▎         | 88/2570 [07:11<3:15:09,  4.72s/it]  3%|▎         | 89/2570 [07:16<3:16:00,  4.74s/it]  4%|▎         | 90/2570 [07:20<3:12:57,  4.67s/it]  4%|▎         | 91/2570 [07:25<3:09:05,  4.58s/it]  4%|▎         | 92/2570 [07:30<3:12:54,  4.67s/it]  4%|▎         | 93/2570 [07:34<3:13:41,  4.69s/it]  4%|▎         | 94/2570 [07:39<3:16:39,  4.77s/it]  4%|▎         | 95/2570 [07:44<3:17:26,  4.79s/it]  4%|▎         | 96/2570 [07:49<3:17:32,  4.79s/it]  4%|▍         | 97/2570 [07:54<3:20:31,  4.87s/it]  4%|▍         | 98/2570 [07:59<3:17:23,  4.79s/it]  4%|▍         | 99/2570 [08:03<3:12:31,  4.68s/it]  4%|▍         | 100/2570 [08:07<3:08:26,  4.58s/it]  4%|▍         | 101/2570 [08:12<3:09:14,  4.60s/it]  4%|▍         | 102/2570 [08:16<3:07:50,  4.57s/it]  4%|▍         | 103/2570 [08:21<3:11:27,  4.66s/it]  4%|▍         | 104/2570 [08:26<3:08:44,  4.59s/it]  4%|▍         | 105/2570 [08:31<3:11:55,  4.67s/it]  4%|▍         | 106/2570 [08:36<3:22:41,  4.94s/it]  4%|▍         | 107/2570 [08:41<3:23:18,  4.95s/it]  4%|▍         | 108/2570 [08:45<3:14:44,  4.75s/it]  4%|▍         | 109/2570 [08:50<3:14:36,  4.74s/it]  4%|▍         | 110/2570 [08:55<3:11:36,  4.67s/it]  4%|▍         | 111/2570 [08:59<3:09:09,  4.62s/it]  4%|▍         | 112/2570 [09:05<3:23:43,  4.97s/it]  4%|▍         | 113/2570 [09:10<3:23:57,  4.98s/it]  4%|▍         | 114/2570 [09:15<3:21:39,  4.93s/it]  4%|▍         | 115/2570 [09:19<3:15:48,  4.79s/it]  5%|▍         | 116/2570 [09:24<3:18:29,  4.85s/it]  5%|▍         | 117/2570 [09:29<3:20:29,  4.90s/it]  5%|▍         | 118/2570 [09:34<3:19:39,  4.89s/it]  5%|▍         | 119/2570 [09:39<3:16:41,  4.82s/it]  5%|▍         | 120/2570 [09:43<3:14:28,  4.76s/it]  5%|▍         | 121/2570 [09:48<3:16:06,  4.80s/it]  5%|▍         | 122/2570 [09:53<3:19:13,  4.88s/it]  5%|▍         | 123/2570 [09:58<3:14:55,  4.78s/it]  5%|▍         | 124/2570 [10:02<3:10:41,  4.68s/it]  5%|▍         | 125/2570 [10:07<3:12:04,  4.71s/it]  5%|▍         | 126/2570 [10:12<3:09:48,  4.66s/it]  5%|▍         | 127/2570 [10:16<3:06:53,  4.59s/it]  5%|▍         | 128/2570 [10:20<3:03:13,  4.50s/it]x1000c3s1b0n1:59117:47641 [0] NCCL INFO Using non-device net plugin version 0
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Using network Socket
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Using non-device net plugin version 0
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Using network Socket
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Using non-device net plugin version 0
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Using network Socket
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Using non-device net plugin version 0
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Using network Socket
x1000c3s1b0n1:59118:47644 [1] NCCL INFO comm 0x56085cde2020 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0x4eadd15c38937d75 - Init START
x1000c3s1b0n1:59117:47641 [0] NCCL INFO comm 0x55ab9fbce750 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0x4eadd15c38937d75 - Init START
x1000c3s1b0n1:59120:47642 [3] NCCL INFO comm 0x564304976920 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0x4eadd15c38937d75 - Init START
x1000c3s1b0n1:59119:47643 [2] NCCL INFO comm 0x55c26e99bf60 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0x4eadd15c38937d75 - Init START
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Setting affinity for GPU 1 to ffff,00000000
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Setting affinity for GPU 3 to ffff
x1000c3s1b0n1:59120:47642 [3] NCCL INFO comm 0x564304976920 rank 3 nRanks 16 nNodes 4 localRanks 4 localRank 3 MNNVL 0
x1000c3s1b0n1:59118:47644 [1] NCCL INFO comm 0x56085cde2020 rank 1 nRanks 16 nNodes 4 localRanks 4 localRank 1 MNNVL 0
x1000c3s1b0n1:59119:47643 [2] NCCL INFO comm 0x55c26e99bf60 rank 2 nRanks 16 nNodes 4 localRanks 4 localRank 2 MNNVL 0
x1000c3s1b0n1:59117:47641 [0] NCCL INFO comm 0x55ab9fbce750 rank 0 nRanks 16 nNodes 4 localRanks 4 localRank 0 MNNVL 0
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Trees [0] 1/-1/-1->3->0 [1] 0/11/-1->3->-1 [2] 1/-1/-1->3->0 [3] 0/-1/-1->3->7
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Trees [0] 2/-1/-1->1->3 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->3 [3] 2/-1/-1->1->0
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Trees [0] -1/-1/-1->2->1 [1] -1/-1/-1->2->1 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
x1000c3s1b0n1:59120:47642 [3] NCCL INFO P2P Chunksize set to 131072
x1000c3s1b0n1:59118:47644 [1] NCCL INFO P2P Chunksize set to 131072
x1000c3s1b0n1:59119:47643 [2] NCCL INFO P2P Chunksize set to 131072
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 01/04 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 03/04 :    0   7   6   5   4  11  10   9   8  15  14  13  12   3   2   1
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Trees [0] 3/8/-1->0->-1 [1] 1/-1/-1->0->3 [2] 3/-1/-1->0->4 [3] 1/-1/-1->0->3
x1000c3s1b0n1:59117:47641 [0] NCCL INFO P2P Chunksize set to 131072
x1000c3s1b0n1:59117:47858 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 00/0 : 15[3] -> 0[0] [receive] via NET/Socket/1
x1000c3s1b0n1:59117:47858 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 02/0 : 15[3] -> 0[0] [receive] via NET/Socket/1
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 02 : 0[0] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Channel 02 : 2[2] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Channel 02 : 1[1] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [send] via NET/Socket/1
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[0] [send] via NET/Socket/1
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[3] [send] via NET/Socket/0
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[3] [send] via NET/Socket/0
x1000c3s1b0n1:59120:47859 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 01/0 : 12[0] -> 3[3] [receive] via NET/Socket/0
x1000c3s1b0n1:59120:47859 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 03/0 : 12[0] -> 3[3] [receive] via NET/Socket/0
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 03 : 3[3] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Channel 03 : 1[1] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Channel 03 : 2[2] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Connected all rings
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Connected all rings
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Connected all rings
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Connected all rings
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 03 : 0[0] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Channel 03 : 1[1] -> 2[2] via SHM/direct/direct
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 00 : 0[0] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Channel 00 : 1[1] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 01 : 0[0] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Channel 02 : 2[2] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Channel 02 : 1[1] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 02 : 0[0] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 03 : 0[0] -> 3[3] via SHM/direct/direct
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 02/0 : 0[0] -> 4[0] [send] via NET/Socket/1
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [send] via NET/Socket/0
x1000c3s1b0n1:59117:47858 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[0] [receive] via NET/Socket/1
x1000c3s1b0n1:59120:47859 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[0] [send] via NET/Socket/1
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [receive] via NET/Socket/0
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [send] via NET/Socket/0
x1000c3s1b0n1:59117:47858 [0] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Channel 02/0 : 4[0] -> 0[0] [receive] via NET/Socket/1
x1000c3s1b0n1:59120:47859 [3] NCCL INFO NET/Socket: Using 8 threads and 2 sockets per thread
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [receive] via NET/Socket/0
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 02 : 3[3] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 03 : 3[3] -> 0[0] via SHM/direct/direct
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 00 : 3[3] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Channel 02 : 3[3] -> 1[1] via SHM/direct/direct
x1000c3s1b0n1:59117:47641 [0] NCCL INFO Connected all trees
x1000c3s1b0n1:59120:47642 [3] NCCL INFO Connected all trees
x1000c3s1b0n1:59117:47641 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s1b0n1:59120:47642 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s1b0n1:59117:47641 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s1b0n1:59120:47642 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s1b0n1:59118:47644 [1] NCCL INFO Connected all trees
x1000c3s1b0n1:59119:47643 [2] NCCL INFO Connected all trees
x1000c3s1b0n1:59118:47644 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s1b0n1:59119:47643 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
x1000c3s1b0n1:59118:47644 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s1b0n1:59119:47643 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
x1000c3s1b0n1:59118:47644 [1] NCCL INFO comm 0x56085cde2020 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 41000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s1b0n1:59120:47642 [3] NCCL INFO comm 0x564304976920 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId c1000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s1b0n1:59117:47641 [0] NCCL INFO comm 0x55ab9fbce750 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 3000 commId 0x4eadd15c38937d75 - Init COMPLETE
x1000c3s1b0n1:59119:47643 [2] NCCL INFO comm 0x55c26e99bf60 rank 2 nranks 16 cudaDev 2 nvmlDev 2 busId 81000 commId 0x4eadd15c38937d75 - Init COMPLETE
                                                    {'loss': 0.9984, 'grad_norm': 0.4304230213165283, 'learning_rate': 4.7658993367147874e-05, 'epoch': 0.5}
  5%|▍         | 128/2570 [10:22<3:03:13,  4.50s/it]  5%|▌         | 129/2570 [10:27<3:25:22,  5.05s/it]  5%|▌         | 130/2570 [10:32<3:22:53,  4.99s/it]  5%|▌         | 131/2570 [10:36<3:18:46,  4.89s/it]  5%|▌         | 132/2570 [10:42<3:34:33,  5.28s/it]  5%|▌         | 133/2570 [10:47<3:28:38,  5.14s/it]  5%|▌         | 134/2570 [10:52<3:20:07,  4.93s/it]  5%|▌         | 135/2570 [10:56<3:16:12,  4.83s/it]  5%|▌         | 136/2570 [11:01<3:13:31,  4.77s/it]  5%|▌         | 137/2570 [11:06<3:11:41,  4.73s/it]  5%|▌         | 138/2570 [11:10<3:11:39,  4.73s/it]  5%|▌         | 139/2570 [11:15<3:10:15,  4.70s/it]  5%|▌         | 140/2570 [11:20<3:10:19,  4.70s/it]  5%|▌         | 141/2570 [11:24<3:09:24,  4.68s/it]  6%|▌         | 142/2570 [11:29<3:06:45,  4.62s/it]  6%|▌         | 143/2570 [11:34<3:17:23,  4.88s/it]  6%|▌         | 144/2570 [11:39<3:11:34,  4.74s/it]  6%|▌         | 145/2570 [11:43<3:08:40,  4.67s/it]  6%|▌         | 146/2570 [11:48<3:07:04,  4.63s/it]  6%|▌         | 147/2570 [11:53<3:09:53,  4.70s/it]  6%|▌         | 148/2570 [11:57<3:06:03,  4.61s/it]  6%|▌         | 149/2570 [12:02<3:09:50,  4.71s/it]  6%|▌         | 150/2570 [12:06<3:06:05,  4.61s/it]  6%|▌         | 151/2570 [12:11<3:11:09,  4.74s/it]  6%|▌         | 152/2570 [12:16<3:12:33,  4.78s/it]  6%|▌         | 153/2570 [12:21<3:15:52,  4.86s/it]  6%|▌         | 154/2570 [12:26<3:09:13,  4.70s/it]  6%|▌         | 155/2570 [12:30<3:10:58,  4.74s/it]  6%|▌         | 156/2570 [12:35<3:07:16,  4.65s/it]  6%|▌         | 157/2570 [12:39<3:05:30,  4.61s/it]slurmstepd: error: *** STEP 1989816.1 ON lanta-g-020 CANCELLED AT 2025-01-06T00:47:02 ***
